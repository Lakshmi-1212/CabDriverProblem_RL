{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T18:48:56.871733Z",
     "start_time": "2021-10-16T18:48:54.197657Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "# for building DQN model \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T18:48:56.877918Z",
     "start_time": "2021-10-16T18:48:56.874301Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T18:48:56.886182Z",
     "start_time": "2021-10-16T18:48:56.881093Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 24, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Time_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T18:48:56.892468Z",
     "start_time": "2021-10-16T18:48:56.888287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Time_matrix[1][2][11][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T18:48:56.900373Z",
     "start_time": "2021-10-16T18:48:56.894822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Time_matrix.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum time to travel between two locations is 11 hours (next state can change by max of 1 day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T18:48:56.905362Z",
     "start_time": "2021-10-16T18:48:56.902433Z"
    }
   },
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO  : Picking the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture 1: input - state, output - q-value for each possible (state, action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T04:09:32.582257Z",
     "start_time": "2021-10-16T04:00:12.028Z"
    }
   },
   "source": [
    "Pros:\n",
    "- Model has to be run once for each state  \n",
    "\n",
    "Cons:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T14:21:21.207406Z",
     "start_time": "2021-10-15T14:21:21.194604Z"
    }
   },
   "source": [
    "#### Architecture 2: input - (state, action), output - q-value for the given (state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T18:48:56.926929Z",
     "start_time": "2021-10-16T18:48:56.910202Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN \n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001       \n",
    "        self.epsilon_max = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.00000001\n",
    "        self.epsilon = 1\n",
    "    \n",
    "        self.explore_count = 0\n",
    "        self.exploit_count = 0\n",
    "        \n",
    "        self.batch_size = 32        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # Initialize the value of the states tracked\n",
    "        self.states_tracked = []\n",
    "        \n",
    "        # Track the state [1,0,0] and action (1,3) at index 7 in the action space.\n",
    "        self.track_state = np.array(env.state_encod_arch1([1,0,0])).reshape(1, 36)\n",
    "        \n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # input layer\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        \n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform')) \n",
    "        \n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform')) \n",
    "        \n",
    "        \n",
    "        \n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "     \n",
    "\n",
    "    def get_action(self, state, episode_count):\n",
    "    # Write your code here:\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    # Decay in Îµ after we generate each sample from the environment       \n",
    "        \n",
    "        \n",
    "#         exp_value = 1/10**(len(str(episode_count))-1)\n",
    "#         self.epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * np.exp(-exp_value*episode_count)\n",
    "    \n",
    "        \n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # exploration: choose a random action from all possible actions\n",
    "            ##print(f\"{episode_count}: EXPLORE\")\n",
    "            self.explore_count += 1\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        else:\n",
    "            # exploitation: choose the action that returns the maximum q-value\n",
    "            ##print(f\"{episode_count}: EXPLOIT\")\n",
    "            \n",
    "            \n",
    "#             # the first index corresponds to the batch size, so\n",
    "#             # reshape state to (1, state_size) so that the first index corresponds to the batch size \n",
    "            state = np.array(env.state_encod_arch1(state)).reshape(1, 36)\n",
    "            #state = np.reshape(state,[1,self.state_size])\n",
    "            q_value = self.model.predict(state) \n",
    "            self.exploit_count += 1\n",
    "            return np.argmax(q_value)\n",
    "         \n",
    " \n",
    "   \n",
    "    def get_summary_details(self):\n",
    "        return self.explore_count, self.exploit_count\n",
    "        \n",
    "\n",
    "        \n",
    "    def reset_episode_counts(self):\n",
    "        self.explore_count = 0\n",
    "        self.exploit_count = 0\n",
    "\n",
    "    def append_sample(self, state, action_idx, reward, next_state, done):\n",
    "    # Write your code here:\n",
    "    # save sample <s,a,r,s',done> to the replay memory after every action\n",
    "        self.memory.append((state, action_idx, reward, next_state, done))\n",
    "        \n",
    "        \n",
    "#         # update Îµ after each sample\n",
    "#         if self.epsilon > self.epsilon_min:\n",
    "#             self.epsilon *= self.epsilon_decay    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        train the neural network on a minibatch. Input to the network is the states,\n",
    "        output is the target q-value corresponding to each action.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            \n",
    "            # sample minibatch from memory\n",
    "            minibatch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "            # initialise two matrices - update_input and update_output\n",
    "            # Initialise the Q(s,a) with zero\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            # Initialise the Q(s',a)\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            actions, rewards, done = [], [], []\n",
    "\n",
    "            # populate update_input and update_output and the lists rewards, actions, done\n",
    "            for i in range(self.batch_size):\n",
    "                state, action_idx, reward, next_state, is_done = minibatch[i]\n",
    "                #print(f'4DEBUG:minibatch[{i}] = {minibatch[i]}')\n",
    "                # Add state s to the Q(s,a), Q(s',a) from memory\n",
    "                #print(f'4DEBUG: state:{state}, encoded state: {env.state_encod_arch1(state)}')\n",
    "                update_input[i] = env.state_encod_arch1(state)     \n",
    "                #update_input[i] = state\n",
    "                # Add action from memory\n",
    "                actions.append(action_idx)\n",
    "                # Add reward from the memory\n",
    "                rewards.append(reward)\n",
    "                # Add next state s' to Q(s',a) from the memory\n",
    "                update_output[i] = env.state_encod_arch1(next_state)\n",
    "                #update_output[i] = next_state\n",
    "                done.append(is_done)\n",
    "\n",
    "            # Find the Q(s,a) and Q(s',a) using state as input to the neural network \n",
    "        \n",
    "            # Predict the target q-values from state s\n",
    "            target = self.model.predict(update_input)\n",
    "\n",
    "            # Get the target for the Q-network\n",
    "            target_qval = self.model.predict(update_output)\n",
    "            \n",
    "            #print(f'4DEBUG: target:{target}')\n",
    "            #print(f'4DEBUG: target_qval:{target_qval}')\n",
    "\n",
    "            # Update the target values - set the target as (r + maxQ(s',a))\n",
    "            for i in range(self.batch_size):\n",
    "                # Q Learning: get maximum Q value at s' from target model\n",
    "                if done[i]:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                else: # non-terminal state\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "\n",
    "            # Train the model\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "     \n",
    "\n",
    "    def save_tracking_states(self):\n",
    "        # Use the model to predict the q_value for the state tracked\n",
    "        q_value = self.model.predict(self.track_state)\n",
    "        print(f'4DEBUG:save_tracking_states - {q_value}')\n",
    "        # Update the q-value for the state tracked\n",
    "        self.states_tracked.append(q_value[0][7])\n",
    "        \n",
    "    def save_model_weights(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T18:48:56.932772Z",
     "start_time": "2021-10-16T18:48:56.930078Z"
    }
   },
   "outputs": [],
   "source": [
    "Episodes = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T18:48:57.016514Z",
     "start_time": "2021-10-16T18:48:56.935926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 36, Action size: 21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = CabDriver()\n",
    "\n",
    "m = 5\n",
    "t = 24\n",
    "d = 7\n",
    "# get size of state and action from environment\n",
    "state_size = m+t+d\n",
    "action_size = len(env.action_space)\n",
    "\n",
    "print(f'State size: {state_size}, Action size: {action_size}')\n",
    "\n",
    "# agent needs to be initialised outside the loop since the DQN\n",
    "# network will be initialised along with the agent\n",
    "agent = DQNAgent(action_size=action_size, state_size=state_size) \n",
    "\n",
    "# to store rewards in each episode\n",
    "rewards_per_episode, episodes = [], []\n",
    "\n",
    "# make dir to store model weights\n",
    "if not os.path.exists(\"saved_model_weights\"):\n",
    "    os.mkdir(\"saved_model_weights\")\n",
    " \n",
    "\n",
    "summary_df = pd.DataFrame(columns=['EPISODE','REWARD','MEMORY_LENGTH','EPSILON','EXPLORE_CNT', 'EXPLOIT_CNT', 'EPISODE_TIME'])\n",
    "\n",
    "summary_threshold = 1000\n",
    "def update_summary_details(episode,reward, memory_len, epsilon, explore_count, exploit_count,episode_time):\n",
    "    #print(f'Updating summary details:{match_results}')\n",
    "    return summary_df.append({'EPISODE' : episode,\n",
    "                       'REWARD' : reward,\n",
    "                       'MEMORY_LENGTH' : memory_len,\n",
    "                       'EPSILON' : epsilon,\n",
    "                       'EXPLORE_CNT' : explore_count,\n",
    "                       'EXPLOIT_CNT' : exploit_count,\n",
    "                       'EPISODE_TIME': episode_time\n",
    "                      }, ignore_index=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T15:56:03.267373Z",
     "start_time": "2021-10-16T15:56:03.263633Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "for episode in range(Episodes):\n",
    "\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    # Call all the initialised variables of the environment\n",
    "    \n",
    "\n",
    "    #Call the DQN agent\n",
    "    \n",
    "    \n",
    "    while !terminal_state:\n",
    "        \n",
    "        # Write your code here\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        # 2. Evaluate your reward and next state\n",
    "        # 3. Append the experience to the memory\n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-16T18:48:54.245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, reward 103.0, memory_length 122, epsilon 0.999, explore:122, exploit:0,time: 7.875\n",
      "Episodes:0 - Elapsed_time:7.877 \n",
      "episode 1, reward -95.0, memory_length 242, epsilon 0.998, explore:120, exploit:0,time: 8.415\n",
      "episode 2, reward 67.0, memory_length 364, epsilon 0.997, explore:122, exploit:0,time: 8.46\n",
      "episode 3, reward -65.0, memory_length 479, epsilon 0.99601, explore:113, exploit:2,time: 8.209\n",
      "episode 4, reward -169.0, memory_length 593, epsilon 0.99501, explore:114, exploit:0,time: 8.117\n",
      "4DEBUG:save_tracking_states - [[    0.    43993.43  42998.03  21654.29  41827.25  42556.1   43334.363\n",
      "      0.    42643.934     0.        0.    42910.95      0.    41047.617\n",
      "  43267.586 42514.113     0.        0.    43234.45  42794.117 43559.688]]\n",
      "episode 5, reward 9.0, memory_length 719, epsilon 0.99401, explore:125, exploit:1,time: 8.733\n",
      "episode 6, reward 107.0, memory_length 840, epsilon 0.99302, explore:120, exploit:1,time: 8.566\n",
      "episode 7, reward -118.0, memory_length 951, epsilon 0.99203, explore:111, exploit:0,time: 7.721\n",
      "episode 8, reward -187.0, memory_length 1060, epsilon 0.99104, explore:109, exploit:0,time: 7.484\n",
      "episode 9, reward -99.0, memory_length 1175, epsilon 0.99004, explore:114, exploit:1,time: 8.094\n",
      "4DEBUG:save_tracking_states - [[     0.   627436.75 623536.9  589595.1  613316.06 600875.44 612608.8\n",
      "       0.   612352.        0.        0.   599671.44      0.   616496.\n",
      "  623559.94 615947.94      0.        0.   614406.75 621134.4  615474.6 ]]\n",
      "episode 10, reward -83.0, memory_length 1301, epsilon 0.98905, explore:125, exploit:1,time: 8.753\n",
      "episode 11, reward -133.0, memory_length 1427, epsilon 0.98807, explore:126, exploit:0,time: 8.84\n",
      "episode 12, reward -259.0, memory_length 1558, epsilon 0.98708, explore:129, exploit:2,time: 9.204\n",
      "episode 13, reward 125.0, memory_length 1684, epsilon 0.98609, explore:125, exploit:1,time: 8.704\n",
      "episode 14, reward -100.0, memory_length 1794, epsilon 0.9851, explore:106, exploit:4,time: 7.851\n",
      "4DEBUG:save_tracking_states - [[      0.   1035815.9  1033523.1  1009716.94 1012598.25 1015399.56\n",
      "  1023865.56       0.   1020843.1        0.         0.   1017270.75\n",
      "        0.   1013145.9  1036157.2  1024314.1        0.         0.\n",
      "  1025570.94 1025996.06 1021889.9 ]]\n",
      "episode 15, reward -86.0, memory_length 1917, epsilon 0.98412, explore:120, exploit:3,time: 8.685\n",
      "episode 16, reward 248.0, memory_length 2000, epsilon 0.98314, explore:121, exploit:0,time: 8.436\n",
      "episode 17, reward -225.0, memory_length 2000, epsilon 0.98215, explore:114, exploit:2,time: 8.209\n",
      "episode 18, reward 305.0, memory_length 2000, epsilon 0.98117, explore:123, exploit:1,time: 8.927\n",
      "episode 19, reward 9.0, memory_length 2000, epsilon 0.98019, explore:123, exploit:2,time: 8.697\n",
      "4DEBUG:save_tracking_states - [[     0.   939387.9  927313.1  936033.94 922032.   913671.8  932058.56\n",
      "       0.   932359.44      0.        0.   932210.06      0.   931690.1\n",
      "  942713.2  934605.5       0.        0.   945073.1  922195.2  940964.44]]\n",
      "episode 20, reward -108.0, memory_length 2000, epsilon 0.97921, explore:104, exploit:5,time: 7.801\n",
      "episode 21, reward -219.0, memory_length 2000, epsilon 0.97823, explore:114, exploit:4,time: 8.246\n",
      "episode 22, reward -360.0, memory_length 2000, epsilon 0.97725, explore:124, exploit:2,time: 8.719\n",
      "episode 23, reward -149.0, memory_length 2000, epsilon 0.97627, explore:121, exploit:3,time: 8.779\n",
      "episode 24, reward -63.0, memory_length 2000, epsilon 0.9753, explore:122, exploit:5,time: 8.925\n",
      "4DEBUG:save_tracking_states - [[     0.   949619.25 945208.8  948979.44 947681.   901787.94 927189.75\n",
      "       0.   951176.5       0.        0.   952607.3       0.   939125.7\n",
      "  952477.1  947958.44      0.        0.   961847.2  953664.5  953621.2 ]]\n",
      "episode 25, reward 225.0, memory_length 2000, epsilon 0.97432, explore:123, exploit:2,time: 8.833\n",
      "episode 26, reward -117.0, memory_length 2000, epsilon 0.97335, explore:112, exploit:4,time: 8.252\n",
      "episode 27, reward -441.0, memory_length 2000, epsilon 0.97237, explore:111, exploit:2,time: 7.856\n",
      "episode 28, reward 30.0, memory_length 2000, epsilon 0.9714, explore:114, exploit:3,time: 8.294\n",
      "episode 29, reward 53.0, memory_length 2000, epsilon 0.97043, explore:115, exploit:2,time: 8.235\n",
      "4DEBUG:save_tracking_states - [[      0.   1000676.6   994652.5   995520.9   985624.2   947976.44\n",
      "   973896.         0.    990242.56       0.         0.   1008522.\n",
      "        0.    979354.8   983037.06  990796.9        0.         0.\n",
      "  1002805.25 1007700.8   997578.2 ]]\n",
      "episode 30, reward 27.0, memory_length 2000, epsilon 0.96946, explore:113, exploit:5,time: 8.32\n",
      "episode 31, reward -32.0, memory_length 2000, epsilon 0.96849, explore:106, exploit:4,time: 8.002\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#### simulation starts ####\n",
    "for episode in range(Episodes):\n",
    "    episode_start_time = time.time()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    # reset at the start of each episode\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        action_idx = agent.get_action(state,episode)\n",
    "        action = env.action_space[action_idx]\n",
    "        # 2. Evaluate your reward and next state\n",
    "        next_state, reward, done = env.step(state, action, Time_matrix)\n",
    "            \n",
    "        #next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # 3. Append the experience to the memory\n",
    "        # save the sample <s, a, r, s', done> to the replay memory\n",
    "        agent.append_sample(state, action_idx, reward, next_state, done)\n",
    "\n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        agent.train_model()\n",
    "\n",
    "        # add reward to the total score of this episode\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "\n",
    "    # epsilon decay\n",
    "    if agent.epsilon > agent.epsilon_min:\n",
    "        agent.epsilon *= agent.epsilon_decay\n",
    "#         agent.epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * np.exp(agent.epsilon_decay*episode)\n",
    "\n",
    "    # 5. Keep a track of rewards, Q-values, loss\n",
    "    # store total reward obtained in this episode\n",
    "    rewards_per_episode.append(score)\n",
    "    episodes.append(episode)\n",
    "    \n",
    "    # Save summary after every episode\n",
    "    explore_count, exploit_count = agent.get_summary_details()\n",
    "    episode_time = time.time() - episode_start_time\n",
    "    summary_df = update_summary_details(episode,score, len(agent.memory),round(agent.epsilon,5),explore_count, exploit_count,round(episode_time,3))\n",
    "    agent.reset_episode_counts()\n",
    "        \n",
    "    # every episode:\n",
    "    print(f\"episode {episode}, reward {score}, memory_length {len(agent.memory)}, epsilon {round(agent.epsilon,5)}, explore:{explore_count}, exploit:{exploit_count},time: {round(episode_time,3)}\")\n",
    "        \n",
    "    # Save the Q_value of the state, action pair we are tracking\n",
    "    if ((episode + 1) % 5 == 0):\n",
    "        agent.save_tracking_states()\n",
    "        \n",
    "        \n",
    "    # every few episodes:\n",
    "    if episode % 100 == 0:\n",
    "        # store q-values of some prespecified state-action pairs\n",
    "        # q_dict = agent.store_q_values()\n",
    "        \n",
    "        \n",
    "        curr_elapsed_time = time.time() - start_time\n",
    "        print(f'Episodes:{episode} - Elapsed_time:{round(curr_elapsed_time,3)} ')  \n",
    "\n",
    "        # save model weights\n",
    "        agent.save_model_weights(name=\"model_weights.h5\")\n",
    "\n",
    " \n",
    "# save model weights\n",
    "agent.save_model_weights(name=\"model_weights.h5\")\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Total elapsed_time:{elapsed_time}')       \n",
    "    \n",
    "        \n",
    "#### simulation complete ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-16T18:48:54.248Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(0, figsize=(16,8))\n",
    "plt.plot(summary_df[['EPISODE','REWARD']])\n",
    "plt.title('REWARDS PER EPISODE')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# save plots in saved_plots/ directory\n",
    "plt.savefig('rewards_episodes.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-16T18:48:54.251Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-16T18:48:54.255Z"
    }
   },
   "outputs": [],
   "source": [
    "# save stuff as pickle\n",
    "def save_pickle(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# make directory\n",
    "if not os.path.exists(\"saved_pickle_files\"):\n",
    "    os.mkdir(\"saved_pickle_files\")\n",
    "\n",
    "# save rewards_per_episode\n",
    "save_pickle(rewards_per_episode, \"saved_pickle_files/rewards_per_episode\")\n",
    "save_pickle(summary_df, \"saved_pickle_files/summary_per_episode\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T18:38:30.937819Z",
     "start_time": "2021-10-16T18:38:30.935420Z"
    }
   },
   "source": [
    "#### Tracked state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-16T18:48:54.258Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.states_tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-16T18:48:54.261Z"
    }
   },
   "outputs": [],
   "source": [
    "state_tracked_sample = [agent.states_tracked[i] for i in range(len(agent.states_tracked)) if agent.states_tracked[i] < 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-value convergence plot for the tracked state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-16T18:48:54.264Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "plt.title('Q_value for state [1,0,0]  action (1,3)')\n",
    "xaxis = np.asarray(range(0, len(agent.states_tracked)))\n",
    "plt.semilogy(xaxis,np.asarray(agent.states_tracked))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Episode vs Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-16T18:48:54.270Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# plot results\n",
    "with open('saved_pickle_files/summary_per_episode.pkl', 'rb') as f:\n",
    "    summary_data = pickle.load(f)\n",
    "\n",
    "    \n",
    "plt.figure(0, figsize=(16,8))\n",
    "plt.plot(summary_data[['EPISODE','REWARD']])\n",
    "plt.title('REWARDS PER EPISODE')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# save plots in saved_plots/ directory\n",
    "plt.savefig('rewards_episodes.png')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-16T18:48:54.273Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# plot results\n",
    "with open('saved_pickle_files/rewards_per_episode.pkl', 'rb') as f:\n",
    "    rewards_per_episode = pickle.load(f)\n",
    "\n",
    "plt.plot(list(range(len(rewards_per_episode))), rewards_per_episode)\n",
    "plt.xlabel(\"episode number\")\n",
    "plt.ylabel(\"reward per episode\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# save plots in saved_plots/ directory\n",
    "plt.savefig('rewards.png')\n",
    "\n",
    "\n",
    "print(\"Average reward of last 100 episodes is {0}\".format(np.mean(rewards_per_episode[-100:]))) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T18:26:13.588362Z",
     "start_time": "2021-10-16T18:26:13.586061Z"
    }
   },
   "source": [
    "#### Plot exploration vs exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-16T18:48:54.276Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(0, figsize=(16,8))\n",
    "plt.plot(summary_data[['EXPLORE_CNT','EXPLOIT_CNT']])\n",
    "plt.title('EXPLORATION vs EXPLOITATION')\n",
    "plt.xlabel(\"Exploration\")\n",
    "plt.ylabel(\"Exploitation\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# save plots in saved_plots/ directory\n",
    "plt.savefig('exploration_exploitation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-16T18:48:54.283Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.0000001\n",
    "episode_count = 25000\n",
    "exp_value = 1/10**(len(str(episode_count))-1)\n",
    "print(f'exp:{exp_value}')\n",
    "time = np.arange(0,episode_count)\n",
    "epsilon = []\n",
    "for i in range(0,episode_count):\n",
    "    epsilon.append(min_epsilon + (max_epsilon - min_epsilon) * np.exp(-exp_value*i))\n",
    "    \n",
    "\n",
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-16T18:48:54.285Z"
    }
   },
   "outputs": [],
   "source": [
    "# From starter code\n",
    "total_episodes = 10000\n",
    "time = np.arange(0,total_episodes)\n",
    "epsilon = []\n",
    "for i in range(0,total_episodes):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))\n",
    "    \n",
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-16T18:48:54.291Z"
    }
   },
   "outputs": [],
   "source": [
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.001\n",
    "episode_count = 50\n",
    "exp_value = 1/10**(len(str(episode_count))-1)\n",
    "time = np.arange(0,episode_count)\n",
    "epsilon = []\n",
    "for i in range(0,episode_count):\n",
    "    epsilon.append(min_epsilon + (max_epsilon - min_epsilon) * np.exp(-exp_value*i))\n",
    "    \n",
    "\n",
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "My Steps",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
